{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e245b40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn\n",
    "import torch.nn.functional as F\n",
    "import tqdm\n",
    "import logging\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch\n",
    "import unicodedata\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "from itertools import chain\n",
    "import time\n",
    "import re\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.modules import dropout\n",
    "from  torch.nn.utils.rnn import pack_padded_sequence\n",
    "from  torch.nn.utils.rnn import pad_packed_sequence\n",
    "import torch.nn.functional as F\n",
    "import sentencepiece as sp\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#all_string=string.ascii_letters+'čḍɛǧɣḥṛṣṭẓ'+'čḍɛǧɣḥṛṣṭẓ'.upper()+'ⴰⴱⵛⴷⴻⴼⴳⵀⵉⵊⴽⵍⵎⵏⵇⵔⵙⵜⵓⵡⵅⵢⵣ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ce8faa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(s):\n",
    "    return re.sub(' +',' ', \"\".join(c if c in string.ascii_letters else \" \"\n",
    "         for c in unicodedata.normalize('NFD', s.lower().strip())\n",
    "         if  c in string.ascii_letters+\" \"+string.punctuation)).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0754974",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170651/170651 [00:03<00:00, 43027.85it/s]\n"
     ]
    }
   ],
   "source": [
    "#########Load File#####################\n",
    "FILE = \"./data/en-fra.txt\"\n",
    "############################################################################################\n",
    "with open(FILE) as f:\n",
    "    lines = f.readlines()\n",
    "original=[]\n",
    "destination=[]\n",
    "for line in tqdm(lines):\n",
    "    for s in line.split(\"\\n\"):\n",
    "        if len(s)<1:continue\n",
    "        orig,dest=map(normalize,s.split(\"\\t\")[:2])\n",
    "        original.append(orig)\n",
    "        destination.append(dest)\n",
    "\n",
    "with open('./data/en.txt', 'w') as f:\n",
    "        for ori in original:\n",
    "            f.write(ori + \"\\n\")\n",
    "\n",
    "with open('./data/fr.txt', 'w') as f:\n",
    "        for dest in destination:\n",
    "            f.write(dest + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70cd442f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-27cb2ab7a3e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#create our English SentencePiece model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSentencePieceTrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"./data/en.txt\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_prefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./data/en'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0men\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSentencePieceProcessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./data/en.model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'spm' is not defined"
     ]
    }
   ],
   "source": [
    "#create our English SentencePiece model\n",
    "sp.SentencePieceTrainer.Train(input=\"./data/en.txt\",model_prefix='./data/en',vocab_size=5000)\n",
    "en = spm.SentencePieceProcessor(model_file='./data/en.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2ead9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp.SentencePieceTrainer.Train(input=\"./data/fr.txt\",model_prefix='./data/fr',vocab_size=5000)\n",
    "fr = spm.SentencePieceProcessor(model_file='./data/fr.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "300765cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TradDataset():\n",
    "    def __init__(self,dataOriginal,dataDestination,vocOrig,vocDest,adding=True,max_len=10):\n",
    "        self.sentences =[]\n",
    "        for orig,dist in zip(dataOriginal,dataDestination):\n",
    "            if len(orig)>max_len: continue\n",
    "            self.sentences.append((torch.tensor(vocOrig.encode(orig, out_type=int)),torch.tensor(vocDest.encode(dist, out_type=int))))\n",
    "    def __len__(self):return len(self.sentences)\n",
    "    def __getitem__(self,i): return self.sentences[i]\n",
    "\n",
    "\n",
    "\n",
    "def collate(batch):\n",
    "    orig,dest = zip(*batch)\n",
    "    o_len = torch.tensor([len(o) for o in orig])\n",
    "    d_len = torch.tensor([len(d) for d in dest])\n",
    "    return pad_sequence(orig),o_len,pad_sequence(dest),d_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d4142de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    #encodeur: un embedding du vocabulaire d'origine puis un gru\n",
    "    def __init__(self, vocab_origine,emb_size, hidden_size):\n",
    "        super(Encoder,self).__init__()\n",
    "        self.vocab_origine=vocab_origine\n",
    "        self.hidden_size=hidden_size\n",
    "        self.emb_size=emb_size\n",
    "        self.embedding = nn.Embedding(vocab_origine, emb_size,padding_idx=PAD)\n",
    "        self.gru = nn.GRU(emb_size, hidden_size)\n",
    "\n",
    "    def forward(self, input,lengths,hidden=None):\n",
    "        embedded = self.embedding(input) #Input pad_sequence\n",
    "        embedded = pack_padded_sequence(embedded,lengths.cpu(),enforce_sorted=False)                                                                                                       \n",
    "        outputs, hidden = self.gru(embedded,hidden)\n",
    "        output,_  = torch.nn.utils.rnn.pad_packed_sequence(outputs) \n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d8ac503",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "\n",
    "    #décodeur : un embedding du vocabulaire de destination, puis un GRU \n",
    "    # suivi d'un réseau linéaire pour le décodage de l'état latent (et un softmax pour terminer)\n",
    "\n",
    "    def __init__(self,vocab_destination,emb_size,hidden_size,bidirectional=False):\n",
    "        super(Decoder,self).__init__()\n",
    "        self.vocab_destination=vocab_destination\n",
    "        self.hidden_size = hidden_size\n",
    "        self.emb_size=emb_size\n",
    "        self.embedding = nn.Embedding(vocab_destination, emb_size,padding_idx=PAD)\n",
    "        self.gru = nn.GRU(emb_size, hidden_size)\n",
    "        self.decoder = nn.Linear(hidden_size  , vocab_destination)\n",
    "    \n",
    "    def forward(self,input,lengths,hidden):\n",
    "        #For teacher forcing\n",
    "        \"\"\"\n",
    "        input: Length x batch \n",
    "        hidden: tensor of shape (1, batch, hidden_size)\n",
    "        \"\"\"\n",
    "        target = torch.tensor([[SOS]*input.shape[1]],device=device) # 1 x Batch\n",
    "        target=self.embedding(target)#1 x Batch x emb_size \n",
    "        _, decoder_hidden = self.gru(target, hidden)  #1 x batch x hidden_size , 1 x batch x hidden_size\n",
    "        target=self.embedding(input)# Length x batch x emb_size\n",
    "        target = pack_padded_sequence(target,lengths.cpu(),enforce_sorted=False)        \n",
    "        outputs,hidden=self.gru(target,decoder_hidden)\n",
    "        output,_  = torch.nn.utils.rnn.pad_packed_sequence(outputs) \n",
    "        return F.log_softmax(self.decoder(output),dim=-1)    #(Length x Batch)\n",
    "\n",
    "    def generate(self,hidden,lenseq=None):\n",
    "        #For constraint mode\n",
    "        \"\"\"\n",
    "        lenseq:length\n",
    "        hidden:1 x Batch x hidden_size\n",
    "        \"\"\"\n",
    "        batch_size=hidden.shape[1]\n",
    "        decoder_outputs = torch.full(size=[lenseq, batch_size, self.vocab_destination], fill_value=PAD,dtype=torch.float,device=device)\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            target = torch.tensor([[SOS]],device=device)\n",
    "            hidden_example=hidden[:,i,:].unsqueeze(1) #1 x 1 x hidden_size\n",
    "            \n",
    "            for j in range(lenseq):\n",
    "                target = self.embedding(target).view(1, 1, -1) # one example and one word -> 1 x 1 x emb_size\n",
    "                output, hidden_example = self.gru(target, hidden_example)\n",
    "                decoder_output = F.log_softmax(self.decoder(hidden_example), dim=-1).squeeze()# vocab_size\n",
    "                indice_max = decoder_output.argmax() #1\n",
    "                target = indice_max.detach() #1\n",
    "                decoder_outputs[j,i]=decoder_output\n",
    "                \n",
    "                if target.item() == EOS:\n",
    "                    break\n",
    "        return decoder_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4ce33058",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "FILE_ORIGINAL = \"./data/en.txt\"\n",
    "FILE_DESTINATION = \"./data/fr.txt\"\n",
    "############################################################################################\n",
    "with open(FILE_ORIGINAL) as f:\n",
    "    dataOriginal = f.readlines()\n",
    "    \n",
    "with open(FILE_DESTINATION) as f:\n",
    "    dataDestination = f.readlines()\n",
    "    \n",
    "############################################################################################ \n",
    "#create our English SentencePiece model\n",
    "sp.SentencePieceTrainer.Train(input=\"./data/en.txt\",model_prefix='./data/en',vocab_size=5000)\n",
    "en = sp.SentencePieceProcessor(model_file='./data/en.model')\n",
    "\n",
    "#create our French SentencePiece model\n",
    "sp.SentencePieceTrainer.Train(input=\"./data/fr.txt\",model_prefix='./data/fr',vocab_size=5000)\n",
    "fr = sp.SentencePieceProcessor(model_file='./data/fr.model')\n",
    "############################################################################################\n",
    "dataOriginal = [dataOriginal[x] for x in torch.randperm(len(dataOriginal))]\n",
    "dataDestination = [dataDestination[x] for x in torch.randperm(len(dataDestination))]\n",
    "idxTrain = int(0.8*len(lines))\n",
    "############################################################################################\n",
    "MAX_LEN=10\n",
    "BATCH_SIZE=64\n",
    "############################################################################################\n",
    "datatrain = TradDataset(dataOriginal[:idxTrain],dataDestination[:idxTrain],en,fr,max_len=MAX_LEN)\n",
    "datatest = TradDataset(dataOriginal[idxTrain:],dataDestination[idxTrain:],en,fr,max_len=MAX_LEN)\n",
    "train_loader = DataLoader(datatrain, collate_fn=collate, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(datatest, collate_fn=collate, batch_size=BATCH_SIZE, shuffle=True)\n",
    "############################################################################################\n",
    "VOCAB_ORIGINE=en.vocab_size()\n",
    "VOCAB_DESTINATION=fr.vocab_size()\n",
    "HIDDEN_SIZE=2048\n",
    "EMB_SIZE=256\n",
    "NB_EPOCH=100\n",
    "\n",
    "PAD = en.pad_id()\n",
    "EOS = en.eos_id()\n",
    "SOS = en.bos_id()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1527c609",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################\n",
    "encoder=Encoder(VOCAB_ORIGINE,EMB_SIZE, HIDDEN_SIZE).to(device)\n",
    "decoder=Decoder(VOCAB_DESTINATION,EMB_SIZE, HIDDEN_SIZE).to(device)\n",
    "############################################################################################\n",
    "loss_fn=nn.NLLLoss(ignore_index=PAD)\n",
    "############################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "29945273",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=optim.RMSprop(chain(encoder.parameters(),decoder.parameters()), lr=0.001)\n",
    "#scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c3d3a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "for i in range(NB_EPOCH):\n",
    "    acc=0\n",
    "    for orig , o_len, dest ,d_len in train_loader:\n",
    "        enc_output, enc_hidden = encoder(orig.to(device), o_len.to(device))\n",
    "        use_teacher_forcing = True if torch.rand(1) < teacher_forcing_ratio else False\n",
    "        \n",
    "        if use_teacher_forcing:\n",
    "            dec_output=decoder(dest.to(device),d_len.to(device),enc_hidden) #OK \n",
    "        else:\n",
    "            dec_output=decoder.generate(enc_hidden,lenseq=max(d_len))\n",
    "        loss = loss_fn(dec_output.permute(0,2,1), dest.to(device))\n",
    "        with torch.no_grad():\n",
    "            acc+=loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Loss/Train {acc/len(train_loader)} a l\"epoche {i}')\n",
    "    #scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94a6960",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.load_state_dict(torch.load('./encoder_saved.pt'))\n",
    "decoder.load_state_dict(torch.load('./decoder_saved.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23f98ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig , o_len, dest ,d_len = next(iter(train_loader))\n",
    "enc_output, enc_hidden = encoder(orig, o_len)\n",
    "dec_output=decoder.generate(enc_hidden,max(d_len)).argmax(dim=-1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83870908",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" \".join(vocEng.getwords(orig[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3773cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "[vocBer.getwords(seq) for seq in dec_output.permute(1,0)][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea143608",
   "metadata": {},
   "outputs": [],
   "source": [
    "\" \" .join(vocBer.getwords(dest[1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
