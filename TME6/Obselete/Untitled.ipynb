{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0d1e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn\n",
    "import torch.nn.functional as F\n",
    "import tqdm\n",
    "import logging\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch\n",
    "import unicodedata\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "from itertools import chain\n",
    "import datetime\n",
    "import time\n",
    "import re\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.modules import dropout\n",
    "from  torch.nn.utils.rnn import pack_padded_sequence\n",
    "from  torch.nn.utils.rnn import pad_packed_sequence\n",
    "import torch.nn.functional as F\n",
    "import sentencepiece as sp\n",
    "from Seq2Seq import * \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "############################################################################################ \n",
    "#create our English SentencePiece model\n",
    "sp.SentencePieceTrainer.Train(input=\"./data/en.txt\",model_prefix='./data/en',vocab_size=7000)\n",
    "en = sp.SentencePieceProcessor(model_file='./data/en.model')\n",
    "\n",
    "#create our French SentencePiece model\n",
    "sp.SentencePieceTrainer.Train(input=\"./data/fr.txt\",model_prefix='./data/fr',vocab_size=7000)\n",
    "fr = sp.SentencePieceProcessor(model_file='./data/fr.model')\n",
    "\n",
    "PAD = en.pad_id()\n",
    "EOS = en.eos_id()\n",
    "SOS = en.bos_id()\n",
    "\n",
    "\n",
    "def normalize(s):\n",
    "    return re.sub(' +',' ', \"\".join(c if c in string.ascii_letters else \" \"\n",
    "         for c in unicodedata.normalize('NFD', s.lower().strip())\n",
    "         if  c in string.ascii_letters+\" \"+string.punctuation)).strip()\n",
    "\n",
    "def fic_splited():\n",
    "    #########Load File#####################\n",
    "    FILE = \"./data/en-fra.txt\"\n",
    "    ############################################################################################\n",
    "    with open(FILE) as f:\n",
    "        lines = f.readlines()\n",
    "    original=[]\n",
    "    destination=[]\n",
    "    for line in tqdm(lines):\n",
    "        for s in line.split(\"\\n\"):\n",
    "            orig,dest=map(normalize,s.split(\"\\t\")[:2])\n",
    "            if len(orig)<1:continue\n",
    "            original.append(orig)\n",
    "            destination.append(dest)\n",
    "\n",
    "    with open('./data/en.txt', 'w') as f:\n",
    "            for ori in original:\n",
    "                f.write(ori + \"\\n\")\n",
    "\n",
    "    with open('./data/fr.txt', 'w') as f:\n",
    "            for dest in destination:\n",
    "                f.write(dest + \"\\n\")\n",
    "\n",
    "\n",
    "class TradDataset():\n",
    "    def __init__(self,dataOriginal,dataDestination,vocOrig,vocDest,adding=True,max_len=10):\n",
    "        self.sentences =[]\n",
    "        for orig,dist in zip(dataOriginal,dataDestination):\n",
    "            if len(orig)<1: continue\n",
    "            if len(orig)>max_len: continue\n",
    "            self.sentences.append((torch.tensor(vocOrig.encode(orig, out_type=int)),torch.tensor(vocDest.encode(dist, out_type=int))))\n",
    "    def __len__(self):return len(self.sentences)\n",
    "    def __getitem__(self,i): return self.sentences[i]\n",
    "\n",
    "\n",
    "\n",
    "def collate(batch):\n",
    "    orig,dest = zip(*batch)\n",
    "    o_len = torch.tensor([len(o) for o in orig])\n",
    "    d_len = torch.tensor([len(d) for d in dest])\n",
    "    return pad_sequence(orig),o_len,pad_sequence(dest),d_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ad0543",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "FILE_ORIGINAL = \"./data/en.txt\"\n",
    "FILE_DESTINATION = \"./data/fr.txt\"\n",
    "############################################################################################\n",
    "with open(FILE_ORIGINAL) as f:\n",
    "    dataOriginal = f.readlines()\n",
    "\n",
    "with open(FILE_DESTINATION) as f:\n",
    "    dataDestination = f.readlines()\n",
    "\n",
    "############################################################################################\n",
    "dataOriginal = [dataOriginal[x] for x in torch.randperm(len(dataOriginal))]\n",
    "dataDestination = [dataDestination[x] for x in torch.randperm(len(dataDestination))]\n",
    "idxTrain = int(0.8*len(dataOriginal))\n",
    "############################################################################################\n",
    "MAX_LEN=5\n",
    "BATCH_SIZE=64\n",
    "############################################################################################\n",
    "datatrain = TradDataset(dataOriginal[:idxTrain],dataDestination[:idxTrain],en,fr,max_len=MAX_LEN)\n",
    "datatest = TradDataset(dataOriginal[idxTrain:],dataDestination[idxTrain:],en,fr,max_len=MAX_LEN)\n",
    "############################################################################################\n",
    "train_loader = DataLoader(datatrain, collate_fn=collate, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(datatest, collate_fn=collate, batch_size=BATCH_SIZE, shuffle=True)\n",
    "############################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cbb7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_ORIGINE=en.vocab_size()\n",
    "VOCAB_DESTINATION=fr.vocab_size()\n",
    "HIDDEN_SIZE=2048\n",
    "EMB_SIZE=256\n",
    "NB_EPOCH=100\n",
    "############################################################################################\n",
    "encoder=Encoder(VOCAB_ORIGINE,EMB_SIZE, HIDDEN_SIZE).to(device)\n",
    "decoder=Decoder(VOCAB_DESTINATION,EMB_SIZE, HIDDEN_SIZE).to(device)\n",
    "############################################################################################\n",
    "loss_fn=nn.NLLLoss(ignore_index=PAD)\n",
    "############################################################################################\n",
    "optimizer=optim.RMSprop(chain(encoder.parameters(),decoder.parameters()), lr=0.001)\n",
    "############################################################################################\n",
    "tf_prob = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed57087",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter('runs/Traduction'+datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c72509",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    for i in range(NB_EPOCH):\n",
    "        acc=0\n",
    "        for orig , o_len, dest ,d_len in train_loader:\n",
    "            enc_output, enc_hidden = encoder(orig.to(device), o_len.to(device))\n",
    "            tf_use = True if torch.rand(1) < tf_prob else False\n",
    "            if tf_use:\n",
    "                dec_output=decoder(dest.to(device),d_len.to(device),enc_hidden) #OK \n",
    "            else:\n",
    "                dec_output=decoder.generate(enc_hidden,lenseq=max(d_len))\n",
    "            loss = loss_fn(dec_output.permute(0,2,1), dest.to(device))\n",
    "            acc+=loss\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f'Loss/Train {acc/len(train_loader)} a l\"epoche {i}')\n",
    "        writer.add_scalar('Loss/train',loss,i)\n",
    "\n",
    "        #Test\n",
    "        with torch.no_grad():\n",
    "            acc=0\n",
    "            for orig , o_len, dest ,d_len in test_loader:\n",
    "                enc_output, enc_hidden = encoder(orig.to(device), o_len.to(device))\n",
    "                dec_output=decoder.generate(enc_hidden,lenseq=max(d_len))\n",
    "                loss = loss_fn(dec_output.permute(0,2,1), dest.to(device))\n",
    "                acc+=loss\n",
    "            print(f'Loss/Test {acc/len(train_loader)} a l\"epoche {i}')\n",
    "            writer.add_scalar('Loss/Test',loss,i)\n",
    "    \n",
    "    #Traduction Example \n",
    "    test_loader = DataLoader(datatest, collate_fn=collate, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    for orig , o_len, dest ,d_len in test_loader:\n",
    "        enc_output, enc_hidden = encoder(orig.to(device), o_len.to(device))\n",
    "        dec_output=decoder.generate(enc_hidden,lenseq=max(d_len)).argmax(dim=-1)\n",
    "        print('Original word=>',\" \".join(en.decode(orig.tolist())))\n",
    "        print('Traduction>',\" \".join(fr.decode(dec_output.tolist())))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
